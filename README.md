Пишем Spring Boot микросервис для деплоя в Kubernetes с нуля!

Напишем REST-сервис с двумя методами:
1) сохранить котика
2) показать всех котиков

Развернём Kubernetes через инструмент kind(?), и развернем наш сервис в кубере.

1. Сначала пишу свой кошачий сервис.

2. БД Postgres в контейнере Докер. Настройки контейнера в файле docker-compose.yaml в корне проекта.

3. Приложение:
http://localhost:8081/

4. Документация апи:
        <dependency>
            <groupId>org.springdoc</groupId>
            <artifactId>springdoc-openapi-ui</artifactId>
            <version>1.6.6</version>
        </dependency>
        
        Адрес: http://localhost:8081/swagger-ui/index.html

5. Далее @Accessors(chain = true)

## Теперь попробуем приложение обернуть в Докер (время 44:10)

1. Создаём Dockerfile в корне.

FROM alpine:3.13
alpine - это минималистичный линукс, в котором нет ничего лишнего
и он максимально мало весит.

RUN apk add openjdk11
На stackoverflow можно увидеть эту команду (загуглил "alpine install jdk11").

Далее надо положить джарник в докер-имейдж
COPY target/cats-api-0.0.1-SNAPSHOT.jar /app.jar
Но сначала джарник надо создать. Выполняю команду: mvn clean package

Далее команды будем выполнять не в консоли, а через скрипт build_and_push.sh
Это сэкономит время, т.к. не надо будет постоянно вводить последовательность одних
и тех же команд руками.

Продолжаем:
COPY target/cats-api-0.0.1-SNAPSHOT.jar /app.jar
Говорим откуда с нашего и хоста, и куда положить на виртуалку (в корень и назвать app.jar).

Теперь укажем с чего должна запуститься виртуальная машина:
ENTRYPOINT ["java", "-jar", "/app.jar"]

Чтобы это всё сбилдить указываем в файле .sh команду
docker build . -t alexz2/cats-api:1.0.0
. значит что будем искать Докерфайл в текущей директории
Через -t указываем имя и версию.

Еще через консоль залогинился на Докер Хабе:
`docker login`
Username: alexz2
Password: <пароль>

Теперь попробуем всё сбилдить: пишу в консоли:  
`chmod +x build_and_push.sh` это чтобы можно было запускать скрипт через консоль. Т.е. сделали
наш файл экзекютабле.

Теперь запускаем:  `./build_and_push.sh`
- собрался джарник
- собрался докер имейдж

Теперь пробуем запустить наш образ: в консоли пишу:
docker run -it --rm alexz2/cats-api:1.0.0
получаем ошибку `org.postgresql.util.PSQLException: Connection to localhost:15432 refused` т.к. приложение 
пытаемся запустить в контейнере, а там нет БД.

Укажем в application.yaml

                  #        url: jdbc:postgresql://localhost:15432/cats_db
                  url: jdbc:postgresql://${DATASOURCE_HOST:localhost}:15432/cats_db

когда мы работаем с Докером, то мы указывваем конфигурации в большинстве случаев
через переменные окружения. На Спринге это делается просто, мы можем прописать переменную окружения DATASOURCE_HOST
и тогда у нас подтянутся наши настройки. Мы можем извне указать хост для БД. В реальной жизни мы также
будем прокидывать логин/пароль, порт и название БД.

Далее удалим образ `docker image rm alexz2/cats-api:1.0.0 `

И опять запустим скрипт .sh

Теперь надо узнать айпишник нашего хоста из контейнера:
`ifconfig`

      enp4s0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
              inet 192.168.1.35  netmask 255.255.255.0  broadcast 192.168.1.255
              inet6 fe80::aef6:2d6f:e51d:8029  prefixlen 64  scopeid 0x20<link>
              ether b4:2e:99:a1:a5:7f  txqueuelen 1000  (Ethernet)
              RX packets 848031  bytes 1163124216 (1.1 GB)
              RX errors 0  dropped 113  overruns 0  frame 0
              TX packets 369179  bytes 34315008 (34.3 MB)
              TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
              device memory 0xfca00000-fca1ffff  

Набираем команду и чуть модернизируем (-e DATASOURCE_HOST=192.168.1.35):
`docker run -it --rm -e DATASOURCE_HOST=192.168.1.35 alexz2/cats-api:1.0.0`

Теперь видим, что всё запустилось.
Даже больше: мы можем прокинуть порт на хост:
-p 8082:8081
`docker run -it --rm -e DATASOURCE_HOST=192.168.1.35 -p 8082:8081 alexz2/cats-api:1.0.0`

Далее в браузере:
http://localhost:8082/api/v1/cat
и вижу что всё работает.

Теперь запушим. Сделаем докер пуш (пропишем в .sh файл)
docker push alexz2/cats-api:1.0.0

И опять в терминале выполним команду:
./build_and_push.sh

Теперь вижу что у меня на Докер Хаб появился новый образ `alexz2/cats-api`

2. Далее мы запустим наш докер имейдж в kind-е.
Идем по адресу 
https://kind.sigs.k8s.io/

Kind - то эмулятор кубернетеса.
Тут надо посмотреть как его установить:

curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64
chmod +x ./kind
mv ./kind ./my_kind/kind

3. Сначала нарисуем, что мы собираемся сделать:

Кубернетес кластер - это набор машин, который обычно состоит из 1 или нескольких **мастеров**,
и **воркеры** (их много). **Мастер-машины** определяют на каких воркерах должно работать
наше приложение, а **воркеры** это уже непосредственно те машины, на которых запускается приложение.

Например хотим запустить наше приложение Cats API в 3 экземплярах.
Запущенное в кубернетес приложение называется **pod**.
cat pod 1 - cp1
cat pod 2 - cp2
cat pod 3 - cp3

В итоге у нас 3 воркера. И каждое приложение на отдельном воркере запустится.

Допустим у нас внутри кубернетес кластера есть еще собачью сервисы:
dog pod 1 - dp1

В итоге у нас есть кластер. В нём 2 приложения:
катс в 3 экземплярах, и догс в 1 экземпляре.

И вот дог хочет сделать запрос на кэтса. Но он не хочет думаьб, в скольки экземплярах
запущен наш кошачий сервис, на каких нодах он запущен, как там распределена нагрузка и т.д..

Специально для этого в кубернетес есть слой который называется **сервис**.
Сервис он как раз распределяет нагрузку между нашими подами.

И если какое то приложение внутри нашего кластера захочет обратиться к кэтс, то оно
может пойти на сервис, а сервис уже сам определит куда обращаться.

Помимо этого у собак может быть свой сервис.

А тепепрь что будет, если придет человек и скажет, не могли бы вы предоставить мне
ответ на мой запрос к кэт сервису? Т.е. прилетит некий запрос из вне. Но чтобы обращаться извне
к кэт сервису, то его надо специальным образом настроить. И для того чтобы сделать 
правильное обращение к нашему кэт сервису извне, у кубернетеса есть специальный плагин
который называется ingres plugin
У него есть разные реализации. Мы возьмём реализацию от nginx-а. А вот уже ingres может
перенаправлять запросы на сервисы. Он чем то похож на сервисы, но если сервисы созданы чтобы распределять
нагрузку, и также используются в качестве сервис-дискавери, т.е. они знают о том где были запущены
наши поды; то ингрес он больше нужен для того чтобы разроутить вот эту вот точку входа, когда
кто-то сторонний заходит в наш кластер, и хочет сделать какой-то запрос. И в итоге клиенту нашего приложения
не надо думать о том, какие там есть сервисы, сколько там подов и т.д.. Клиент просто знает что по урлу
/api/v1/cats  он получит одну информацию, а по другому урлу он получит другую информацию. И ему ничего
не надо знать о внутреннем механизме кубера.

Сейчас реализуем часть, связанную с кошачьим сервисом.

Зачем всё это нужно? Почему кубернетес так удобен? Когда мы всё это поднимем, то кубер сам будет
определять на какой машине всё это запустить. Сам организует нам сервис, и т.д.
Если бы не было кубера, то нам бы самим пришлось деплоить каждый раз на какой-то сервис,
думать насчёт сервис-дискавери, ну и т.д.. Например мы работали в 3 инстансах, а потом решили что в 5 будет лучше.
Для кубера это ещё одна простая команда. Без кубера нам пришлось бы долго возиться с этим.

4. Теперь осуществим всё это на практике (01:08:00).

Сначала установим такую утилиту как

